"""
è‡ªç„¶è¯­è¨€å¤„ç†æœåŠ¡ - é€šä¹‰åƒé—® + RAG
"""

import asyncio
import logging
from typing import Dict, List, Optional, Tuple, Any
import json
from datetime import datetime
import re

# é€šä¹‰åƒé—®ç›¸å…³
from dashscope import Generation
import dashscope

# LangChainç›¸å…³
from langchain.vectorstores import Milvus
from langchain.embeddings import DashScopeEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import Document

from ..utils.config import settings
from .knowledge_service import KnowledgeService

logger = logging.getLogger(__name__)

class NLPService:
    """è‡ªç„¶è¯­è¨€å¤„ç†æœåŠ¡"""
    
    def __init__(self):
        # é…ç½®é€šä¹‰åƒé—®
        dashscope.api_key = settings.DASHSCOPE_API_KEY
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = DashScopeEmbeddings(
            model="text-embedding-v1",
            dashscope_api_key=settings.DASHSCOPE_API_KEY
        )
        
        # å‘é‡æ•°æ®åº“è¿æ¥
        self.vectorstore = None
        
        # çŸ¥è¯†åº“æœåŠ¡
        self.knowledge_service = KnowledgeService()
        
        # å¯¹è¯è®°å¿†
        self.memory = ConversationBufferWindowMemory(
            k=10,  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            memory_key="chat_history",
            return_messages=True
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ"]
        )
        
        logger.info("é€šä¹‰åƒé—®NLPæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def check_health(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        return bool(settings.DASHSCOPE_API_KEY)
    
    async def initialize_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥"""
        try:
            self.vectorstore = Milvus(
                embedding_function=self.embeddings,
                connection_args={
                    "host": settings.MILVUS_HOST,
                    "port": settings.MILVUS_PORT
                },
                collection_name="socialwise_knowledge"
            )
            logger.info("Milvuså‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Milvusè¿æ¥å¤±è´¥: {str(e)}")
            raise
    
    async def process_query(self, question: str, session_id: str, chat_history: List = None) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        try:
            # 1. æ„å›¾è¯†åˆ«å’Œé—®é¢˜åˆ†ç±»
            intent_result = await self._classify_intent(question)
            
            # 2. æ ¹æ®æ„å›¾é€‰æ‹©å¤„ç†ç­–ç•¥
            if intent_result["intent"] == "trusted_qa":
                # ä¼˜å…ˆä½¿ç”¨å¯ä¿¡QAå¯¹
                answer_result = await self._query_trusted_qa(question)
            elif intent_result["intent"] == "faq":
                # ä½¿ç”¨FAQ
                answer_result = await self._query_faq(question)
            else:
                # ä½¿ç”¨RAGæ£€ç´¢åŸå§‹æ–‡æ¡£
                answer_result = await self._query_with_rag(question, chat_history or [])
            
            # 3. åå¤„ç†å’Œä¼˜åŒ–
            final_answer = await self._post_process_answer(
                question=question,
                answer=answer_result["answer"],
                sources=answer_result.get("sources", [])
            )
            
            return {
                "answer": final_answer,
                "confidence": answer_result.get("confidence", 0.8),
                "sources": answer_result.get("sources", []),
                "intent": intent_result["intent"],
                "processing_time": answer_result.get("processing_time", 0)
            }
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚",
                "confidence": 0.0,
                "sources": [],
                "intent": "error",
                "processing_time": 0
            }
    
    async def _classify_intent(self, question: str) -> Dict[str, Any]:
        """æ„å›¾è¯†åˆ«å’Œé—®é¢˜åˆ†ç±»"""
        try:
            # ä½¿ç”¨é€šä¹‰åƒé—®è¿›è¡Œæ„å›¾åˆ†ç±»
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜çš„æ„å›¾ç±»å‹ï¼Œä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªï¼š

1. trusted_qa - å¯ä»¥ç”¨å·²ç¡®è®¤çš„é—®ç­”å¯¹ç›´æ¥å›ç­”çš„é—®é¢˜
2. faq - å¸¸è§é—®é¢˜ï¼Œå¯ä»¥ç”¨FAQå›ç­”
3. document_search - éœ€è¦ä»æ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯çš„å¤æ‚é—®é¢˜
4. calculation - éœ€è¦è®¡ç®—çš„é—®é¢˜ï¼ˆå¦‚ç¤¾ä¿ç¼´è´¹è®¡ç®—ï¼‰
5. policy_inquiry - æ”¿ç­–å’¨è¯¢ç±»é—®é¢˜
6. procedure_inquiry - åŠäº‹æµç¨‹ç±»é—®é¢˜

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åªè¿”å›ç±»åˆ«åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            
            response = Generation.call(
                model="qwen-turbo",
                prompt=prompt,
                max_tokens=50
            )
            
            intent = response.output.text.strip().lower()
            
            # éªŒè¯æ„å›¾ç±»åˆ«
            valid_intents = ["trusted_qa", "faq", "document_search", "calculation", "policy_inquiry", "procedure_inquiry"]
            if intent not in valid_intents:
                intent = "document_search"  # é»˜è®¤ä½¿ç”¨æ–‡æ¡£æœç´¢
            
            return {
                "intent": intent,
                "confidence": 0.9
            }
            
        except Exception as e:
            logger.error(f"æ„å›¾è¯†åˆ«å¤±è´¥: {str(e)}")
            return {
                "intent": "document_search",
                "confidence": 0.5
            }
    
    async def _query_trusted_qa(self, question: str) -> Dict[str, Any]:
        """æŸ¥è¯¢å¯ä¿¡QAå¯¹"""
        try:
            # ä»æ•°æ®åº“æŸ¥è¯¢å¯ä¿¡QAå¯¹
            trusted_qa = await self.knowledge_service.search_trusted_qa(question, limit=3)
            
            if trusted_qa:
                # ä½¿ç”¨æœ€åŒ¹é…çš„QAå¯¹
                best_match = trusted_qa[0]
                return {
                    "answer": best_match["answer"],
                    "confidence": best_match["similarity"],
                    "sources": [f"å¯ä¿¡é—®ç­”å¯¹ ID: {best_match['id']}"],
                    "processing_time": 0.1
                }
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„å¯ä¿¡QAå¯¹ï¼Œé™çº§åˆ°FAQ
                return await self._query_faq(question)
                
        except Exception as e:
            logger.error(f"å¯ä¿¡QAæŸ¥è¯¢å¤±è´¥: {str(e)}")
            return await self._query_faq(question)
    
    async def _query_faq(self, question: str) -> Dict[str, Any]:
        """æŸ¥è¯¢FAQ"""
        try:
            # ä»æ•°æ®åº“æŸ¥è¯¢FAQ
            faq_results = await self.knowledge_service.search_faq(question, limit=3)
            
            if faq_results:
                # ä½¿ç”¨æœ€åŒ¹é…çš„FAQ
                best_match = faq_results[0]
                return {
                    "answer": best_match["answer"],
                    "confidence": best_match["similarity"],
                    "sources": [f"FAQ ID: {best_match['id']}"],
                    "processing_time": 0.2
                }
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„FAQï¼Œä½¿ç”¨RAG
                return await self._query_with_rag(question, [])
                
        except Exception as e:
            logger.error(f"FAQæŸ¥è¯¢å¤±è´¥: {str(e)}")
            return await self._query_with_rag(question, [])
    
    async def _query_with_rag(self, question: str, chat_history: List) -> Dict[str, Any]:
        """ä½¿ç”¨RAGæ£€ç´¢å›ç­”"""
        try:
            start_time = datetime.now()
            
            # ç¡®ä¿å‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–
            if not self.vectorstore:
                await self.initialize_vectorstore()
            
            # å‘é‡æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            
            relevant_docs = await retriever.aget_relevant_documents(question)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            
            # æ„å»ºå¯¹è¯å†å²
            history_text = ""
            if chat_history:
                for msg in chat_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
                    role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
                    history_text += f"{role}: {msg.get('content', '')}\n"
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""
ä½ æ˜¯"ç¤¾ä¿æ™ºç­”/SocialWise"æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”ç¤¾ä¼šä¿éšœå’Œç¦åˆ©æœåŠ¡ç›¸å…³é—®é¢˜ã€‚

å¯¹è¯å†å²ï¼š
{history_text}

ç›¸å…³çŸ¥è¯†ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºæä¾›çš„çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¦æ±‚ï¼š
1. å›ç­”å‡†ç¡®ã€ä¸“ä¸šã€æ˜“æ‡‚
2. å¦‚æœçŸ¥è¯†ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜
3. å›ç­”è¦æœ‰æ¡ç†ï¼Œå¿…è¦æ—¶ä½¿ç”¨ç¼–å·æˆ–åˆ†ç‚¹
4. è¯­æ°”å‹å¥½ã€ä¸“ä¸š
5. å›ç­”é•¿åº¦æ§åˆ¶åœ¨200å­—ä»¥å†…

å›ç­”ï¼š
"""
            
            # è°ƒç”¨é€šä¹‰åƒé—®ç”Ÿæˆå›ç­”
            response = Generation.call(
                model="qwen-max",
                prompt=prompt,
                max_tokens=512,
                temperature=0.1
            )
            
            answer = response.output.text.strip()
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æå–æºæ–‡æ¡£ä¿¡æ¯
            sources = []
            for doc in relevant_docs:
                if hasattr(doc, 'metadata') and doc.metadata:
                    source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                    sources.append(source)
            
            return {
                "answer": answer,
                "confidence": 0.8,
                "sources": list(set(sources)),  # å»é‡
                "processing_time": processing_time
            }
            
        except Exception as e:
            logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {str(e)}")
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                "confidence": 0.0,
                "sources": [],
                "processing_time": 0
            }
    
    async def _post_process_answer(self, question: str, answer: str, sources: List[str]) -> str:
        """åå¤„ç†å’Œä¼˜åŒ–å›ç­”"""
        try:
            # 1. æ¸…ç†æ ¼å¼
            answer = re.sub(r'\n+', '\n', answer)  # åˆå¹¶å¤šä¸ªæ¢è¡Œ
            answer = answer.strip()
            
            # 2. æ·»åŠ å‹å¥½çš„å¼€å¤´ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not any(greeting in answer[:20] for greeting in ["æ‚¨å¥½", "ä½ å¥½", "æ ¹æ®", "å…³äº"]):
                if "æŸ¥è¯¢" in question or "æ€ä¹ˆ" in question:
                    answer = f"å…³äºæ‚¨çš„é—®é¢˜ï¼Œ{answer}"
            
            # 3. æ·»åŠ æ¥æºä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if sources and len(sources) > 0:
                source_text = "ã€".join(sources[:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ¥æº
                answer += f"\n\nå‚è€ƒæ¥æºï¼š{source_text}"
            
            # 4. æ·»åŠ å“ç‰Œæ ‡è¯†
            if len(answer) > 100:  # åªåœ¨è¾ƒé•¿å›ç­”ä¸­æ·»åŠ 
                answer += "\n\n---\nğŸ’¡ ç¤¾ä¿æ™ºç­”/SocialWise ä¸ºæ‚¨æœåŠ¡"
            
            return answer
            
        except Exception as e:
            logger.error(f"å›ç­”åå¤„ç†å¤±è´¥: {str(e)}")
            return answer
    
    async def generate_qa_pairs(self, document_text: str) -> List[Dict[str, str]]:
        """ä»æ–‡æ¡£ç”Ÿæˆé—®ç­”å¯¹"""
        try:
            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_text(document_text)
            
            qa_pairs = []
            
            for chunk in chunks:
                if len(chunk.strip()) < 50:  # è·³è¿‡å¤ªçŸ­çš„ç‰‡æ®µ
                    continue
                
                # ä½¿ç”¨é€šä¹‰åƒé—®ç”Ÿæˆé—®ç­”å¯¹
                prompt = f"""
åŸºäºä»¥ä¸‹ç¤¾ä¿ç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆ3-5ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{chunk}

è¦æ±‚ï¼š
1. é—®é¢˜è¦å…·ä½“ã€å®ç”¨ï¼Œæ˜¯ç”¨æˆ·å¯èƒ½ä¼šé—®çš„
2. ç­”æ¡ˆè¦å‡†ç¡®ã€å®Œæ•´ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹
3. æ ¼å¼ï¼šQ: é—®é¢˜\nA: ç­”æ¡ˆ\n\n
4. æ¯ä¸ªé—®ç­”å¯¹ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”

ç”Ÿæˆçš„é—®ç­”å¯¹ï¼š
"""
                
                response = Generation.call(
                    model="qwen-max",
                    prompt=prompt,
                    max_tokens=1024,
                    temperature=0.3
                )
                
                # è§£æç”Ÿæˆçš„é—®ç­”å¯¹
                generated_text = response.output.text.strip()
                pairs = self._parse_qa_pairs(generated_text)
                qa_pairs.extend(pairs)
            
            logger.info(f"ä»æ–‡æ¡£ç”Ÿæˆäº† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
            return qa_pairs
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆé—®ç­”å¯¹å¤±è´¥: {str(e)}")
            return []
    
    def _parse_qa_pairs(self, text: str) -> List[Dict[str, str]]:
        """è§£æç”Ÿæˆçš„é—®ç­”å¯¹æ–‡æœ¬"""
        pairs = []
        
        try:
            # æŒ‰Q:å’ŒA:åˆ†å‰²
            sections = re.split(r'Q:|A:', text)
            
            for i in range(1, len(sections), 2):
                if i + 1 < len(sections):
                    question = sections[i].strip()
                    answer = sections[i + 1].strip()
                    
                    if question and answer:
                        pairs.append({
                            "question": question,
                            "answer": answer,
                            "source": "generated",
                            "confidence": 0.7
                        })
            
        except Exception as e:
            logger.error(f"è§£æé—®ç­”å¯¹å¤±è´¥: {str(e)}")
        
        return pairs
    
    async def evaluate_answer_quality(self, question: str, answer: str) -> Dict[str, Any]:
        """è¯„ä¼°å›ç­”è´¨é‡"""
        try:
            prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹é—®ç­”çš„è´¨é‡ï¼Œä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦æ‰“åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š

é—®é¢˜ï¼š{question}
å›ç­”ï¼š{answer}

è¯„ä¼°ç»´åº¦ï¼š
1. å‡†ç¡®æ€§ - å›ç­”æ˜¯å¦å‡†ç¡®
2. å®Œæ•´æ€§ - å›ç­”æ˜¯å¦å®Œæ•´
3. ç›¸å…³æ€§ - å›ç­”æ˜¯å¦ä¸é—®é¢˜ç›¸å…³
4. æ¸…æ™°åº¦ - å›ç­”æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
5. å®ç”¨æ€§ - å›ç­”æ˜¯å¦å®ç”¨

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{"accuracy": åˆ†æ•°, "completeness": åˆ†æ•°, "relevance": åˆ†æ•°, "clarity": åˆ†æ•°, "usefulness": åˆ†æ•°, "overall": æ€»åˆ†, "feedback": "å…·ä½“åé¦ˆ"}}
"""
            
            response = Generation.call(
                model="qwen-turbo",
                prompt=prompt,
                max_tokens=256
            )
            
            # è§£æè¯„ä¼°ç»“æœ
            result_text = response.output.text.strip()
            evaluation = json.loads(result_text)
            
            return evaluation
            
        except Exception as e:
            logger.error(f"å›ç­”è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return {
                "accuracy": 7,
                "completeness": 7,
                "relevance": 7,
                "clarity": 7,
                "usefulness": 7,
                "overall": 7,
                "feedback": "è¯„ä¼°å¤±è´¥"
            }